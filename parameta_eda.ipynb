{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f343aa-38cb-4705-955e-b28d6cf85e6b",
   "metadata": {},
   "source": [
    "### Problem 1 – the rates problem\n",
    "\n",
    "You have three data files, rates_ccy_data.csv, rates_price_data.parq.gzip and rates_spot_rate_data.parq.gzip\n",
    "\n",
    "rates_ccy_data is a currency pair reference file. It tells us the currency pairs (ccy_pair) in scope, whether we need to convert the price, and the conversion factor. Price in this example relates to the FX rate for the ccy_pair.\n",
    "\n",
    "rates_price_data is a set of timestamped prices for ccy_pairs.\n",
    "\n",
    "rates_spot_rate_data is a timestamped set of fx rates for ccy_pairs. The FX rates are in a column called ‘spot_mid_rate’.\n",
    "\n",
    "Goal: Generate a new price for each row in rates_price_data. The new price depends on whether that ccy_pair is supported, needs to be converted and has sufficient data to convert:\n",
    "\n",
    "If conversion is not required, then the new price is simply the ‘existing price’\n",
    "If conversion is required, then the new price is: (‘existing price’/ ‘conversion factor’) + ‘spot_mid_rate’\n",
    "If there is insufficient data to create a new price then capture this fact in some way\n",
    " \n",
    "For each row in rates_price_data, if conversion is required then we need to find an appropriate ‘spot_mid_rate’. This is defined as the most recently timestamped rate for that specific ccy_pair within the hour that precedes the timestamped price.\n",
    "\n",
    "Please generate the final price for each row in price_data. You can capture this as an output data file – csv is preferable.\n",
    "\n",
    "For reference, the benchmark calculation time is < 1 second using python 3 with 16GB ram\n",
    "\n",
    "\n",
    "#### Personal notes\n",
    "\n",
    "New price has 2 failure conditions:\n",
    "1. ccy_pair not supported, i.e. not found in rates_ccy\n",
    "2. insufficient data to convert, i.e. no data in rates_price_data within the preceding one hour window\n",
    "\n",
    "Flow should go like this:\n",
    "1. is ccy_pair supported? if no, np.nan, otherwise proceed to next step\n",
    "2. is conversion required? if no, existing price is used. if yes, proceed to next step.\n",
    "3. is there sufficient data to convert? if no, np.nan. otherwise price/conversion_factor + spot_mid_rate\n",
    "\n",
    "This results in 4 potential cases, 2 cases of np.nan, 1 case of unchanged price as the new price, 1 case of new price as computed with the formula above.\n",
    "\n",
    "Ideally we should capture the cause of the NaN, therefore I will be adding a comments column.\n",
    "\n",
    "Rough code outline:\n",
    "\n",
    "1. Join rates_price_data with rates_ccy on ccy_pair, unsupported pairs will have NaN for rates_ccy data\n",
    "2. Sort timestamp data for new rates_price and rates_spot, and then use pd.merge_asof, which is a merge by key distance, with a 1 hour tolerance. If tolerance not met, rates_spot data will be NaN\n",
    "3. Compute new price, existing NaN in rates_ccy and rates_spot data will result in NaN in new price\n",
    "4. If conversion_factor == False, set new_price column to original price, this will get us all 4 cases done in new_price column\n",
    "5. Use conditional masks to identify fail cases for both unsupported ccy_pair and insufficient spot rate data, create a comments column\n",
    "\n",
    "Validation checks:\n",
    "\n",
    "1. Unique ccy_pair in rates_ccy_data? The merge will result in multiplicatively more rows for duplicated ccy_pair\n",
    "2. If conversion_factor == False, new_price == price?\n",
    "3. If conversion_factor == NaN, new_price == NaN & comment == 'unsupported ccy_pair'?\n",
    "4. If spot_mid_rate == NaN, new_price == NaN & comment == 'no preceding data for spot rates'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471def3f-0519-4981-9828-88b7f73ee183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "rates_ccy = pd.read_csv(\"rates_ccy_data.csv\")\n",
    "rates_spot = pd.read_parquet(\"rates_spot_rate_data.parq.gzip\")\n",
    "rates_price = pd.read_parquet(\"rates_price_data.parq.gzip\")\n",
    "std_price = pd.read_parquet(\"stdev_price_data.parq.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e4b53-a7a2-4cc6-857b-df5c1b0bed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if ccy_pair is a unique identifier for each row\n",
    "rates_ccy['ccy_pair'].nunique() == rates_ccy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7debc1-17ac-4bcc-8f50-a5ac7cbf4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join onto rates_price, results in np.nan if unsupported ccy_pair\n",
    "rates = pd.merge(rates_price,rates_ccy,on='ccy_pair',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b76adc-4ed7-4544-8e67-0d949ef9f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort timestamps and convert to datetime64 datatype\n",
    "rates[\"timestamp\"] = pd.to_datetime(rates[\"timestamp\"])\n",
    "rates = rates.sort_values(\"timestamp\")\n",
    "rates_spot[\"timestamp\"] = pd.to_datetime(rates_spot[\"timestamp\"])\n",
    "rates_spot = rates_spot.sort_values(\"timestamp\")\n",
    "# Duplicate timestamp in reference dataframe for later use\n",
    "rates_spot[\"timestamp_preceding\"] = rates_spot[\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75d119-d408-4ab4-aa50-7f0c5a9a1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join with 1 hour tolerance as per requirements\n",
    "# Takes first timestamp before due to sorting\n",
    "rates_new = pd.merge_asof(rates,rates_spot,on=\"timestamp\",by='ccy_pair',direction=\"backward\",tolerance=pd.Timedelta(\"1h\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c6f8e-df82-457b-8118-0edb469378dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new prices with the given formula\n",
    "rates_new['new_price'] = rates_new['price']/rates_new['conversion_factor'] + rates_new[\"spot_mid_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b24f8-f772-40d7-b43a-89ae84f626e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override np.nan new_price where conversion is not required, due to np.nan in conversion_factor\n",
    "rates_new.loc[rates_new[\"convert_price\"]==False, \"new_price\"] = rates_new.loc[rates_new[\"convert_price\"]==False, \"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed73db-83c8-410b-ab26-36519a8d3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comments\n",
    "cond_1 = rates_new[\"convert_price\"].isna()\n",
    "rates_new.loc[cond_1, \"comments\"] = \"ccy_pair not supported\"\n",
    "cond_2 = rates_new[\"spot_mid_rate\"].isna() & ~cond_1\n",
    "rates_new.loc[cond_2, \"comments\"] = \"no preceding data for spot rates\"\n",
    "rates_new[\"comments\"] = rates_new[\"comments\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9514199-84b0-4004-8ea7-ba5dc90a2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_new.to_csv(\"output.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f5436-fb0b-4840-948d-2a181b14aa1b",
   "metadata": {},
   "source": [
    "### Problem 2 – the standard deviation problem\n",
    "\n",
    "You have a single gzip compressed parquet file called stdev_price_data.parq.gzip\n",
    "\n",
    "It consists of timestamped ‘bid’, ‘mid’ and ‘ask’ prices for security IDs. These prices are generated in hourly snaps. For each ‘security_id’ at all possible hourly snap times in the interval defined below, we need to know the rolling standard deviation for bids, for mids and for asks. In other words, for each snap hour, generate the rolling standard deviation for each of bids, mids and asks for each ID.\n",
    "\n",
    "To generate a standard deviation for a security id at a given hourly snap time, you need the most recent set of 20 contiguous hourly snap values for the security id. By contiguous we mean there are no gaps in the set of hourly snaps.\n",
    "\n",
    "Please generate the standard deviation of price for each price type for each security for every possible hourly snap from 2021-11-20 00:00:00 to 2021-11-23 09:00:00. You can capture this as an output data file – csv is preferable\n",
    "\n",
    "Some thoughts – in this problem, consider yourself at snap point in time. Look back at previous data and work out the stdev. An hour later, you are at the next snap time, and you calculate the new stdev. You can think of this as a) updating based on previous data or b) as a completely new calculation.\n",
    "\n",
    "In a), you could consider storing stdev calculation state data at each snap time and using that at the next snap time.\n",
    "\n",
    "In b), you ignore calculation data from previous steps and do the calculation afresh each time.\n",
    "\n",
    "Although you only need to calculate from 20th Nov for a few days, in reality we run this calculation every hour and so your solution should be able to handle a request to show the result on any given hour. You are not required to support this but it should help guide your solution.\n",
    "\n",
    "Although you are working with files, in reality we would be pulling this data from a database and caching any intermediate or state data to local (file) or remote (database) storage so the idea of having state data that can be accessed in the future is not unusual. For example, if someone queries our result we need to be able to easily regenerate it.\n",
    "\n",
    "For reference, the benchmark calculation time is again < 1 second using python 3 with 16GB ram\n",
    "\n",
    "#### Personal notes\n",
    "\n",
    "At each point in time, if the data for the preceding 20 hours exist, the standard deviation is computed using \"the square root of the average of the squared deviations of the values subtracted from their average value\". This can then be cached alongside a few other variables for future piecemeal computations***, which can then be performed more efficiently similar to memoisation used in dynamic programming (for the purpose of solution a mentioned). This requires distinctively different code from the initial bulk computation (solution b). The code required are as follows:\n",
    "\n",
    "Initial bulk compute (solution b):\n",
    "\n",
    "1. Partition std_price into security id, and sort by snap_time\n",
    "2. Identify contiguous sequences of 1 hour and breaks via feature engineering\n",
    "3. Leverage pandas functions for vectorised computation\n",
    "\n",
    "Incremental piecemeal compute (solution a)***:\n",
    "\n",
    "1. From previous computations, store the first value of the N (in this case, N=20) window, along with the rolling mean and standard deviation\n",
    "2. Compute new rolling mean using previous rolling mean + (new value - previous first value)/N\n",
    "3. Compute new rolling standard deviation using previous standard deviation + (new value - previous first value) * (new value - new rolling mean + previous first value - old rolling mean)\n",
    "\n",
    "*** I have referred to this blog (https://jonisalonen.com/2014/efficient-and-accurate-rolling-standard-deviation/) for the algorithm, but this has not been implemented.\n",
    "\n",
    "Validation checks:\n",
    "\n",
    "1. No duplicate snap_time?\n",
    "2. snap_time should be sorted correctly\n",
    "3. Check for contiguity, should have 20 contiguous values\n",
    "4. Stdev should be > 0\n",
    "\n",
    "Amended solution:\n",
    "\n",
    "The main requirement is to generate stdev for a given time period, in this case being 2021-11-20 00:00:00 to 2021-11-23 09:00:00, computed using contiguous time values or the most recent contiguous set of values, which would require forward filling. \n",
    "\n",
    "My first idea is to generate all hourly intervals with pd.date_range, and then left join from the initial computed dataframe before forward filling. This does have the problem of resulting in NaN values if the first hourly interval is not part of a contiguous chain BUT has recent contiguous values to draw from. \n",
    "\n",
    "Therefore, there is also the more computationally heavy option of simply fully computing all hourly intervals given the date range of the entire dataset, however this would scale poorly given extremely large date ranges. Hence, my solution would involve adding the latest available filled stdev for each security (just 1 row), and then removing it later as part of post-processing (just a simple pandas filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000882-2787-4fcc-b532-c22a1f371a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_price = std_price.sort_values([\"security_id\", \"snap_time\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca300457-02f0-4ef0-8b24-79fea219bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate row-to-row time deltas\n",
    "time_diffs = std_price.groupby(\"security_id\")[\"snap_time\"].diff()\n",
    "\n",
    "# Feature engineer a column to show at which points the contiguity is broken\n",
    "std_price[\"contiguity_broken\"] = (time_diffs != pd.Timedelta(\"1h\")) & time_diffs.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a80804-d5c8-4449-9814-8d43c862cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previous feature engineered column, identify new sequence within each security_id\n",
    "std_price[\"sequence_id\"] = std_price.groupby(\"security_id\")[\"contiguity_broken\"].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ebdd7c-b0e3-4444-bf25-d518edcaae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorised operations with pandas to generate stdev\n",
    "for col in [\"bid\", \"mid\", \"ask\"]:\n",
    "    std_price[f\"{col}_stdev\"] = std_price.groupby([\"security_id\", \"sequence_id\"])[col].rolling(window=20, min_periods=20).std().reset_index(level=[0,1], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eaa8f-6287-4700-90a3-f8cc0de09235",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdev_cols = [\"bid_stdev\", \"mid_stdev\", \"ask_stdev\"]\n",
    "\n",
    "std_price[stdev_cols] = (\n",
    "    std_price\n",
    "    .groupby([\"security_id\"])[stdev_cols]\n",
    "    .ffill()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22ca7b-68c2-4eed-90ee-2fb3f7b7d2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = pd.Timestamp(\"2021-11-20 00:00:00\")\n",
    "END_TIME = pd.Timestamp(\"2021-11-23 09:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30b2c4-b6e7-4cd8-bc2f-91f00df687a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = pd.date_range(START_TIME, END_TIME, freq=\"h\")\n",
    "\n",
    "security_ids = std_price[\"security_id\"].unique()\n",
    "\n",
    "output_cols = [\"snap_time\",\"security_id\", *stdev_cols]\n",
    "\n",
    "hourly_intervals = (\n",
    "    pd.MultiIndex.from_product(\n",
    "        [hours,security_ids],\n",
    "        names=[\"snap_time\",\"security_id\"],\n",
    "    )\n",
    "    .to_frame(index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27646d25-e0bb-49e4-ab89-20589c35f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_intervals = pd.merge(\n",
    "    hourly_intervals,\n",
    "    std_price[output_cols],\n",
    "    on=[\"security_id\", \"snap_time\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cea10-8245-489c-af42-018cc526443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_value = (\n",
    "    std_price[std_price[\"snap_time\"] < START_TIME]\n",
    "    .sort_values(\"snap_time\")\n",
    "    .groupby(\"security_id\")\n",
    "    .tail(1)\n",
    ")[[\"security_id\", \"snap_time\", *stdev_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d19e0f-69c8-460e-a606-538a32ece350",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat(\n",
    "    [\n",
    "        last_value,\n",
    "        hourly_intervals,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").sort_values([\"security_id\", \"snap_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d2cfe-1677-4a64-9869-f71b391234f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[stdev_cols] = (\n",
    "    output.groupby(\"security_id\")[stdev_cols].ffill()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584503a-6b94-4baf-b39b-ddffcb9ff44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[output[\"snap_time\"] >= START_TIME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837990ca-7b40-4f2b-b4ee-c76ff1189dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output[\"security_id\"]==\"id_44\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b82ae-b166-4800-a02d-a26a43d31149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
